{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b2364b2-7b0b-467f-95bb-238f9a409b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\v.mikheev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\v.mikheev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\v.mikheev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "nltk.download(\"stopwords\")\n",
    "from keras.layers import Embedding\n",
    "import pymorphy2\n",
    "#morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1fda79b-4fcd-465e-8c2c-8b58cfe38bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.17.0-cp312-cp312-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting tensorflow-intel==2.17.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.17.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading grpcio-1.66.2-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.5.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\v.mikheev\\appdata\\local\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.17.0-cp312-cp312-win_amd64.whl (2.0 kB)\n",
      "Downloading tensorflow_intel-2.17.0-cp312-cp312-win_amd64.whl (385.2 MB)\n",
      "   ---------------------------------------- 0.0/385.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/385.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/385.2 MB 6.4 MB/s eta 0:01:00\n",
      "   ---------------------------------------- 3.9/385.2 MB 7.8 MB/s eta 0:00:49\n",
      "    --------------------------------------- 7.3/385.2 MB 10.3 MB/s eta 0:00:37\n",
      "    --------------------------------------- 8.1/385.2 MB 8.8 MB/s eta 0:00:43\n",
      "   - -------------------------------------- 10.0/385.2 MB 9.0 MB/s eta 0:00:42\n",
      "   - -------------------------------------- 11.8/385.2 MB 8.9 MB/s eta 0:00:42\n",
      "   - -------------------------------------- 13.6/385.2 MB 8.9 MB/s eta 0:00:42\n",
      "   - -------------------------------------- 15.7/385.2 MB 9.0 MB/s eta 0:00:42\n",
      "   - -------------------------------------- 17.6/385.2 MB 9.0 MB/s eta 0:00:41\n",
      "   -- ------------------------------------- 19.7/385.2 MB 9.1 MB/s eta 0:00:41\n",
      "   -- ------------------------------------- 21.8/385.2 MB 9.1 MB/s eta 0:00:40\n",
      "   -- ------------------------------------- 23.6/385.2 MB 9.2 MB/s eta 0:00:40\n",
      "   -- ------------------------------------- 25.7/385.2 MB 9.2 MB/s eta 0:00:39\n",
      "   -- ------------------------------------- 27.8/385.2 MB 9.3 MB/s eta 0:00:39\n",
      "   --- ------------------------------------ 29.9/385.2 MB 9.3 MB/s eta 0:00:39\n",
      "   --- ------------------------------------ 32.0/385.2 MB 9.4 MB/s eta 0:00:38\n",
      "   --- ------------------------------------ 34.1/385.2 MB 9.4 MB/s eta 0:00:38\n",
      "   --- ------------------------------------ 36.4/385.2 MB 9.5 MB/s eta 0:00:37\n",
      "   ---- ----------------------------------- 38.5/385.2 MB 9.5 MB/s eta 0:00:37\n",
      "   ---- ----------------------------------- 40.6/385.2 MB 9.6 MB/s eta 0:00:37\n",
      "   ---- ----------------------------------- 42.7/385.2 MB 9.6 MB/s eta 0:00:36\n",
      "   ---- ----------------------------------- 45.1/385.2 MB 9.6 MB/s eta 0:00:36\n",
      "   ---- ----------------------------------- 47.4/385.2 MB 9.6 MB/s eta 0:00:36\n",
      "   ----- ---------------------------------- 49.5/385.2 MB 9.7 MB/s eta 0:00:35\n",
      "   ----- ---------------------------------- 51.6/385.2 MB 9.7 MB/s eta 0:00:35\n",
      "   ----- ---------------------------------- 53.7/385.2 MB 9.7 MB/s eta 0:00:34\n",
      "   ----- ---------------------------------- 55.8/385.2 MB 9.8 MB/s eta 0:00:34\n",
      "   ------ --------------------------------- 58.2/385.2 MB 9.8 MB/s eta 0:00:34\n",
      "   ------ --------------------------------- 60.6/385.2 MB 9.8 MB/s eta 0:00:34\n",
      "   ------ --------------------------------- 62.7/385.2 MB 9.8 MB/s eta 0:00:33\n",
      "   ------ --------------------------------- 65.0/385.2 MB 9.9 MB/s eta 0:00:33\n",
      "   ------ --------------------------------- 67.1/385.2 MB 9.9 MB/s eta 0:00:33\n",
      "   ------- -------------------------------- 69.5/385.2 MB 9.9 MB/s eta 0:00:32\n",
      "   ------- -------------------------------- 71.6/385.2 MB 9.9 MB/s eta 0:00:32\n",
      "   ------- -------------------------------- 73.9/385.2 MB 9.9 MB/s eta 0:00:32\n",
      "   ------- -------------------------------- 76.0/385.2 MB 9.9 MB/s eta 0:00:32\n",
      "   -------- ------------------------------- 78.1/385.2 MB 9.9 MB/s eta 0:00:31\n",
      "   -------- ------------------------------- 80.2/385.2 MB 10.0 MB/s eta 0:00:31\n",
      "   -------- ------------------------------- 82.3/385.2 MB 10.0 MB/s eta 0:00:31\n",
      "   -------- ------------------------------- 84.4/385.2 MB 10.0 MB/s eta 0:00:31\n",
      "   -------- ------------------------------- 86.5/385.2 MB 10.0 MB/s eta 0:00:30\n",
      "   --------- ------------------------------ 88.9/385.2 MB 10.0 MB/s eta 0:00:30\n",
      "   --------- ------------------------------ 91.0/385.2 MB 10.0 MB/s eta 0:00:30\n",
      "   --------- ------------------------------ 93.3/385.2 MB 10.0 MB/s eta 0:00:30\n",
      "   --------- ------------------------------ 95.7/385.2 MB 10.0 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 98.0/385.2 MB 10.1 MB/s eta 0:00:29\n",
      "   ---------- ---------------------------- 100.1/385.2 MB 10.1 MB/s eta 0:00:29\n",
      "   ---------- ---------------------------- 102.5/385.2 MB 10.1 MB/s eta 0:00:29\n",
      "   ---------- ---------------------------- 104.9/385.2 MB 10.1 MB/s eta 0:00:28\n",
      "   ---------- ---------------------------- 107.2/385.2 MB 10.1 MB/s eta 0:00:28\n",
      "   ----------- --------------------------- 109.6/385.2 MB 10.1 MB/s eta 0:00:28\n",
      "   ----------- --------------------------- 111.9/385.2 MB 10.2 MB/s eta 0:00:27\n",
      "   ----------- --------------------------- 114.3/385.2 MB 10.2 MB/s eta 0:00:27\n",
      "   ----------- --------------------------- 116.4/385.2 MB 10.2 MB/s eta 0:00:27\n",
      "   ------------ -------------------------- 118.8/385.2 MB 10.2 MB/s eta 0:00:27\n",
      "   ------------ -------------------------- 120.8/385.2 MB 10.2 MB/s eta 0:00:26\n",
      "   ------------ -------------------------- 122.7/385.2 MB 10.2 MB/s eta 0:00:26\n",
      "   ------------ -------------------------- 124.3/385.2 MB 10.1 MB/s eta 0:00:26\n",
      "   ------------ -------------------------- 126.4/385.2 MB 10.1 MB/s eta 0:00:26\n",
      "   ------------ -------------------------- 128.2/385.2 MB 10.1 MB/s eta 0:00:26\n",
      "   ------------- ------------------------- 130.3/385.2 MB 10.1 MB/s eta 0:00:26\n",
      "   ------------- ------------------------- 132.4/385.2 MB 10.1 MB/s eta 0:00:26\n",
      "   ------------- ------------------------- 134.5/385.2 MB 10.1 MB/s eta 0:00:25\n",
      "   ------------- ------------------------- 136.6/385.2 MB 10.1 MB/s eta 0:00:25\n",
      "   -------------- ------------------------ 138.9/385.2 MB 10.1 MB/s eta 0:00:25\n",
      "   -------------- ------------------------ 140.8/385.2 MB 10.1 MB/s eta 0:00:25\n",
      "   -------------- ------------------------ 142.9/385.2 MB 10.1 MB/s eta 0:00:24\n",
      "   -------------- ------------------------ 145.2/385.2 MB 10.1 MB/s eta 0:00:24\n",
      "   -------------- ------------------------ 147.6/385.2 MB 10.1 MB/s eta 0:00:24\n",
      "   --------------- ----------------------- 149.9/385.2 MB 10.1 MB/s eta 0:00:24\n",
      "   --------------- ----------------------- 152.3/385.2 MB 10.1 MB/s eta 0:00:23\n",
      "   --------------- ----------------------- 154.4/385.2 MB 10.1 MB/s eta 0:00:23\n",
      "   --------------- ----------------------- 157.0/385.2 MB 10.2 MB/s eta 0:00:23\n",
      "   ---------------- ---------------------- 159.4/385.2 MB 10.2 MB/s eta 0:00:23\n",
      "   ---------------- ---------------------- 161.5/385.2 MB 10.2 MB/s eta 0:00:22\n",
      "   ---------------- ---------------------- 164.1/385.2 MB 10.2 MB/s eta 0:00:22\n",
      "   ---------------- ---------------------- 166.5/385.2 MB 10.2 MB/s eta 0:00:22\n",
      "   ----------------- --------------------- 168.8/385.2 MB 10.2 MB/s eta 0:00:22\n",
      "   ----------------- --------------------- 171.2/385.2 MB 10.2 MB/s eta 0:00:21\n",
      "   ----------------- --------------------- 173.5/385.2 MB 10.3 MB/s eta 0:00:21\n",
      "   ----------------- --------------------- 176.2/385.2 MB 10.3 MB/s eta 0:00:21\n",
      "   ------------------ -------------------- 178.5/385.2 MB 10.3 MB/s eta 0:00:21\n",
      "   ------------------ -------------------- 180.9/385.2 MB 10.3 MB/s eta 0:00:20\n",
      "   ------------------ -------------------- 183.2/385.2 MB 10.3 MB/s eta 0:00:20\n",
      "   ------------------ -------------------- 185.9/385.2 MB 10.3 MB/s eta 0:00:20\n",
      "   ------------------- ------------------- 188.2/385.2 MB 10.3 MB/s eta 0:00:20\n",
      "   ------------------- ------------------- 190.8/385.2 MB 10.4 MB/s eta 0:00:19\n",
      "   ------------------- ------------------- 193.2/385.2 MB 10.4 MB/s eta 0:00:19\n",
      "   ------------------- ------------------- 195.6/385.2 MB 10.4 MB/s eta 0:00:19\n",
      "   -------------------- ------------------ 197.9/385.2 MB 10.4 MB/s eta 0:00:19\n",
      "   -------------------- ------------------ 200.3/385.2 MB 10.4 MB/s eta 0:00:18\n",
      "   -------------------- ------------------ 202.9/385.2 MB 10.4 MB/s eta 0:00:18\n",
      "   -------------------- ------------------ 205.5/385.2 MB 10.4 MB/s eta 0:00:18\n",
      "   --------------------- ----------------- 207.9/385.2 MB 10.4 MB/s eta 0:00:17\n",
      "   --------------------- ----------------- 210.2/385.2 MB 10.4 MB/s eta 0:00:17\n",
      "   --------------------- ----------------- 212.6/385.2 MB 10.5 MB/s eta 0:00:17\n",
      "   --------------------- ----------------- 215.0/385.2 MB 10.5 MB/s eta 0:00:17\n",
      "   ---------------------- ---------------- 217.3/385.2 MB 10.5 MB/s eta 0:00:17\n",
      "   ---------------------- ---------------- 219.9/385.2 MB 10.5 MB/s eta 0:00:16\n",
      "   ---------------------- ---------------- 222.3/385.2 MB 10.5 MB/s eta 0:00:16\n",
      "   ---------------------- ---------------- 224.7/385.2 MB 10.5 MB/s eta 0:00:16\n",
      "   ---------------------- ---------------- 227.0/385.2 MB 10.5 MB/s eta 0:00:16\n",
      "   ----------------------- --------------- 229.6/385.2 MB 10.5 MB/s eta 0:00:15\n",
      "   ----------------------- --------------- 232.0/385.2 MB 10.5 MB/s eta 0:00:15\n",
      "   ----------------------- --------------- 234.6/385.2 MB 10.5 MB/s eta 0:00:15\n",
      "   ----------------------- --------------- 237.0/385.2 MB 10.6 MB/s eta 0:00:15\n",
      "   ------------------------ -------------- 239.3/385.2 MB 10.6 MB/s eta 0:00:14\n",
      "   ------------------------ -------------- 241.7/385.2 MB 10.6 MB/s eta 0:00:14\n",
      "   ------------------------ -------------- 244.1/385.2 MB 10.6 MB/s eta 0:00:14\n",
      "   ------------------------ -------------- 246.4/385.2 MB 10.6 MB/s eta 0:00:14\n",
      "   ------------------------- ------------- 249.0/385.2 MB 10.6 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 251.7/385.2 MB 10.6 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 254.0/385.2 MB 10.6 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 256.6/385.2 MB 10.6 MB/s eta 0:00:13\n",
      "   -------------------------- ------------ 259.0/385.2 MB 10.6 MB/s eta 0:00:12\n",
      "   -------------------------- ------------ 261.6/385.2 MB 10.6 MB/s eta 0:00:12\n",
      "   -------------------------- ------------ 264.0/385.2 MB 10.7 MB/s eta 0:00:12\n",
      "   -------------------------- ------------ 266.6/385.2 MB 10.7 MB/s eta 0:00:12\n",
      "   --------------------------- ----------- 269.2/385.2 MB 10.7 MB/s eta 0:00:11\n",
      "   --------------------------- ----------- 271.8/385.2 MB 10.8 MB/s eta 0:00:11\n",
      "   --------------------------- ----------- 274.2/385.2 MB 10.8 MB/s eta 0:00:11\n",
      "   ---------------------------- ---------- 276.8/385.2 MB 10.8 MB/s eta 0:00:11\n",
      "   ---------------------------- ---------- 279.2/385.2 MB 10.8 MB/s eta 0:00:10\n",
      "   ---------------------------- ---------- 281.5/385.2 MB 10.8 MB/s eta 0:00:10\n",
      "   ---------------------------- ---------- 283.9/385.2 MB 10.9 MB/s eta 0:00:10\n",
      "   ---------------------------- ---------- 286.3/385.2 MB 10.9 MB/s eta 0:00:10\n",
      "   ----------------------------- --------- 288.6/385.2 MB 10.9 MB/s eta 0:00:09\n",
      "   ----------------------------- --------- 291.0/385.2 MB 10.9 MB/s eta 0:00:09\n",
      "   ----------------------------- --------- 293.6/385.2 MB 10.9 MB/s eta 0:00:09\n",
      "   ----------------------------- --------- 296.2/385.2 MB 10.9 MB/s eta 0:00:09\n",
      "   ------------------------------ -------- 298.6/385.2 MB 10.9 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 300.9/385.2 MB 11.0 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 303.6/385.2 MB 11.0 MB/s eta 0:00:08\n",
      "   ------------------------------- ------- 306.2/385.2 MB 11.0 MB/s eta 0:00:08\n",
      "   ------------------------------- ------- 308.5/385.2 MB 11.0 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 310.9/385.2 MB 11.0 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 313.5/385.2 MB 11.0 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 315.9/385.2 MB 11.0 MB/s eta 0:00:07\n",
      "   -------------------------------- ------ 318.5/385.2 MB 11.1 MB/s eta 0:00:07\n",
      "   -------------------------------- ------ 320.9/385.2 MB 11.1 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 323.5/385.2 MB 11.1 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 325.8/385.2 MB 11.1 MB/s eta 0:00:06\n",
      "   --------------------------------- ----- 328.5/385.2 MB 11.1 MB/s eta 0:00:06\n",
      "   --------------------------------- ----- 331.1/385.2 MB 11.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 333.4/385.2 MB 11.1 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 336.1/385.2 MB 11.2 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 338.4/385.2 MB 11.2 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 341.0/385.2 MB 11.2 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 343.4/385.2 MB 11.2 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 345.8/385.2 MB 11.2 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 347.9/385.2 MB 11.2 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 349.7/385.2 MB 11.2 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 352.1/385.2 MB 11.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 353.9/385.2 MB 11.2 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 355.7/385.2 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 357.6/385.2 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 359.7/385.2 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 361.5/385.2 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 363.6/385.2 MB 11.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 365.4/385.2 MB 11.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 367.5/385.2 MB 11.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 369.4/385.2 MB 11.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 371.5/385.2 MB 11.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 373.6/385.2 MB 11.0 MB/s eta 0:00:02\n",
      "   --------------------------------------  375.4/385.2 MB 11.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.5/385.2 MB 11.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  379.6/385.2 MB 11.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  381.7/385.2 MB 10.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  383.8/385.2 MB 11.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.1/385.2 MB 11.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.1/385.2 MB 11.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.1/385.2 MB 11.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.1/385.2 MB 11.0 MB/s eta 0:00:01\n",
      "   --------------------------------------- 385.2/385.2 MB 10.6 MB/s eta 0:00:00\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.66.2-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 2.1/4.3 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.9/4.3 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 8.9 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.1/26.4 MB 9.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.9/26.4 MB 9.8 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 6.0/26.4 MB 9.7 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 8.1/26.4 MB 9.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 10.2/26.4 MB 9.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 12.3/26.4 MB 9.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 14.4/26.4 MB 9.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 16.5/26.4 MB 9.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 18.6/26.4 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 20.7/26.4 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 23.1/26.4 MB 9.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.2/26.4 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.2/26.4 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 9.5 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl (127 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 2.1/5.5 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.9/5.5 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 9.3 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Installing collected packages: libclang, flatbuffers, termcolor, tensorboard-data-server, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml_dtypes 0.5.0\n",
      "    Uninstalling ml_dtypes-0.5.0:\n",
      "      Successfully uninstalled ml_dtypes-0.5.0\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.66.2 libclang-18.1.1 ml-dtypes-0.4.1 opt-einsum-3.4.0 tensorboard-2.17.1 tensorboard-data-server-0.7.2 tensorflow-2.17.0 tensorflow-intel-2.17.0 termcolor-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e22bb10-6968-428b-ba71-9df6a008ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pymorphy2_311_hotfix():\n",
    "    from inspect import getfullargspec\n",
    "    from pymorphy2.units.base import BaseAnalyzerUnit\n",
    "\n",
    "    def _get_param_names_311(klass):\n",
    "        if klass.__init__ is object.__init__:\n",
    "            return []\n",
    "        args = getfullargspec(klass.__init__).args\n",
    "        return sorted(args[1:])\n",
    "\n",
    "    setattr(BaseAnalyzerUnit, '_get_param_names', _get_param_names_311)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f0e5a03-853b-48b1-8a8e-be9a36d46a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pymorphy2_311_hotfix()\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff1eb68a-76a0-4a68-bfeb-b4f3f5017e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open('C:/Users/v.mikheev/Downloads/intentsbars.json',encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98f900a0-7734-48a1-b899-6ad3158e5964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizem(text):\n",
    "    words = text.split() # разбиваем текст на слова\n",
    "    res = list()\n",
    "    for word in words:\n",
    "        p = morph.parse(word)[0]\n",
    "        res.append(p.normal_form)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51c646b3-f10d-49bb-bd48-cd3ba2768105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "intents = json.loads(data_file,strict=False)\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!',',']+ russian_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb676d47-3c32-4720-9c90-69d1c6d1f528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 documents\n",
      "28 unique lemmatized words\n",
      "6 classes ['DWH', 'ETL', 'Задачи Alpha BI', 'Метод пузырька', 'Регламенты и инструкции общего характера', 'Туалет']\n"
     ]
    }
   ],
   "source": [
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "\n",
    "        # take each word and tokenize it\n",
    "        w = lemmatizem(pattern)\n",
    "        words.extend(w)\n",
    "\n",
    "        # adding documents\n",
    "        documents.append((w, intent['tag']))\n",
    "\n",
    "        # adding classes to our class list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(words), \"unique lemmatized words\")\n",
    "print (len(classes), \"classes\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c5cbab1-52b0-4c8f-a90b-037dd775e041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alpha', 'bi', 'dwh', 'etl', 'python', 'барс', 'груп', 'задача', 'инструкция', 'комната', 'метод', 'нужный', 'общий', 'предложить', 'простой', 'процесс', 'пузырёк', 'работа', 'рассказать', 'регламент', 'режим', 'решать', 'сортировка', 'туалет', 'уборный', 'узнать', 'характер', 'хотеть']\n"
     ]
    }
   ],
   "source": [
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "for doc in documents:\n",
    "\n",
    "    # initializing bag of words\n",
    "    bag = []\n",
    "\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "\n",
    "    # lemmatize each word - create base word, in attempt to represent related words\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "\n",
    "    # create our bag of words array with 1, if word match found in current pattern\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.asarray(training, dtype=\"object\")\n",
    "#training = np.array(training)\n",
    "\n",
    "# create train and test lists. X - patterns, Y - intents\n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ceb820e7-385b-4599-9ba4-68e8be642be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "for doc in documents:\n",
    "\n",
    "    # initializing bag of words\n",
    "    bag = []\n",
    "\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "\n",
    "    # lemmatize each word - create base word, in attempt to represent related words\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "\n",
    "    # create our bag of words array with 1, if word match found in current pattern\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.asarray(training, dtype=\"object\")\n",
    "#training = np.array(training)\n",
    "\n",
    "# create train and test lists. X - patterns, Y - intents\n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ccc855a-bb9a-44c0-8fd6-28d54bb59f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v.mikheev\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
    "# equal to number of intents to predict output intent with softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(64,input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
    "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ceb6b60a-013f-4540-b4fe-a9fdff4444f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.1250 - loss: 1.8861  \n",
      "Epoch 2/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1667 - loss: 1.8392 \n",
      "Epoch 3/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 1.8654 \n",
      "Epoch 4/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2000 - loss: 1.8119 \n",
      "Epoch 5/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3333 - loss: 1.6848 \n",
      "Epoch 6/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.8018 \n",
      "Epoch 7/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.6632 \n",
      "Epoch 8/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2750 - loss: 1.6132     \n",
      "Epoch 9/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4083 - loss: 1.5134 \n",
      "Epoch 10/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3667 - loss: 1.6832 \n",
      "Epoch 11/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7083 - loss: 1.5590 \n",
      "Epoch 12/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5833 - loss: 1.4723 \n",
      "Epoch 13/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7667 - loss: 1.3976 \n",
      "Epoch 14/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6833 - loss: 1.3385 \n",
      "Epoch 15/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8000 - loss: 1.2397 \n",
      "Epoch 16/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6000 - loss: 1.2623 \n",
      "Epoch 17/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6917 - loss: 1.0643 \n",
      "Epoch 18/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7333 - loss: 0.9515 \n",
      "Epoch 19/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7500 - loss: 1.0046 \n",
      "Epoch 20/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7500 - loss: 1.0028 \n",
      "Epoch 21/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7167 - loss: 1.1343 \n",
      "Epoch 22/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6083 - loss: 1.0931 \n",
      "Epoch 23/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8333 - loss: 0.8882 \n",
      "Epoch 24/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5750 - loss: 1.0973 \n",
      "Epoch 25/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7417 - loss: 0.8780 \n",
      "Epoch 26/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8750 - loss: 0.7001 \n",
      "Epoch 27/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7917 - loss: 0.6213 \n",
      "Epoch 28/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7833 - loss: 0.6960 \n",
      "Epoch 29/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9083 - loss: 0.4627 \n",
      "Epoch 30/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8917 - loss: 0.5659 \n",
      "Epoch 31/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9083 - loss: 0.4109 \n",
      "Epoch 32/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9083 - loss: 0.4760 \n",
      "Epoch 33/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.4530 \n",
      "Epoch 34/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8000 - loss: 0.4740 \n",
      "Epoch 35/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9667 - loss: 0.4582 \n",
      "Epoch 36/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.3978 \n",
      "Epoch 37/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8333 - loss: 0.3742 \n",
      "Epoch 38/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8583 - loss: 0.5142 \n",
      "Epoch 39/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.2458 \n",
      "Epoch 40/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.3014 \n",
      "Epoch 41/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.2624 \n",
      "Epoch 42/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8000 - loss: 0.5307 \n",
      "Epoch 43/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8583 - loss: 0.3681 \n",
      "Epoch 44/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.2617 \n",
      "Epoch 45/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.1262 \n",
      "Epoch 46/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.1687 \n",
      "Epoch 47/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.3006 \n",
      "Epoch 48/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.2075 \n",
      "Epoch 49/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.1378 \n",
      "Epoch 50/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8917 - loss: 0.2526 \n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=50, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0928eda-08c6-41b2-b8e0-e939a70f24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c49d539c-1ef4-430d-ac83-077a38fa84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(sentence, words, show_details=True):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "\n",
    "    # bag of words - matrix of N words, vocabulary matrix\n",
    "    bag = [0] * len(words)\n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "\n",
    "                # assign 1 if current word is in the vocabulary position\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c60dba7e-a530-47ec-8d66-a7bb6fdb59b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(sentence, model):\n",
    "    # filter out predictions below a threshold\n",
    "    p = bow(sentence, words,show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6dd2aa1a-c26f-40ef-a9c8-3d31300dfff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResponse(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag']== tag):\n",
    "            result = random.choice(i['responses'])\n",
    "            print(result)\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ad9876b-51ed-4212-9301-c73ff821a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_response(msg):\n",
    "    ints = predict_class(msg, model)\n",
    "    res = getResponse(ints, intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c793b2e-9475-44d2-ba37-05bf2432f1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "что такое? dwh\n"
     ]
    }
   ],
   "source": [
    "query = 'Что такое? DWH'\n",
    "res = lemmatizem(query)\n",
    "query = ' '.join(res)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39388644-8318-4395-b393-b3d4529092d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "Data Warehouse (DWH) — хранилище, предназначенное для сбора и аналитической обработки исторических данных организации.\n",
      "Позволяет объединить данные из различных систем и источников в одном месте, а также улучшить качество данных и скорость доступа к ним.\n",
      "Про DWH в Alpha BI можно узнать по ссылке https://conf.bars.group/pages/viewpage.action?pageId=49586442\n"
     ]
    }
   ],
   "source": [
    "chatbot_response(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5a53c-2285-4fb6-8ac4-6850d5fdbc06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
